{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python numpy tensorflow keras matplotlib scikit-image pytesseract imutils scipy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FMeuRj3A1nM",
        "outputId": "b18be46b-1ae5-42c4-e964-73094fd96b38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TErnSlYeBACP",
        "outputId": "8c4d90f5-4fb2-48dc-a91d-fde4b4623b2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVHYsq1oAyL1",
        "outputId": "c4f81c40-fc3c-43dd-fb49-9fc0d271bb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.3.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exDy0xcuFB8F",
        "outputId": "f5fc30fd-7cad-4fb0-c3e0-df4d400fc0c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update package index\n",
        "!apt-get update\n",
        "\n",
        "# 2. Fix missing issues and install poppler-utils\n",
        "!apt-get install -y --fix-missing poppler-utils\n",
        "\n",
        "# 3. Confirm poppler is installed\n",
        "!pdftoppm -v\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DzdpMzdFG1x",
        "outputId": "e23df141-cfd1-47ff-f747-4e217f6ab07a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,775 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,207 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,103 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,290 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,168 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,518 kB]\n",
            "Fetched 32.3 MB in 7s (4,422 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\n",
            "Fetched 186 kB in 0s (404 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "pdftoppm version 22.02.0\n",
            "Copyright 2005-2022 The Poppler Developers - http://poppler.freedesktop.org\n",
            "Copyright 1996-2011 Glyph & Cog, LLC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9ZW1vBFJ3q",
        "outputId": "eb906fc4-8eb1-4ad8-e584-dc7b242c9c5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from pdf2image import convert_from_path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Reshape, Bidirectional, LSTM, Lambda, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HandwrittenOCR:\n",
        "    def __init__(self, student_scripts_path, ground_truth_path):\n",
        "        self.student_scripts_path = student_scripts_path\n",
        "        self.ground_truth_path = ground_truth_path\n",
        "\n",
        "        # Define comprehensive character set\n",
        "        self.char_list = [''] + sorted(list(\n",
        "            set(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,!?;:'\\\"()-/\\\\&@#$%^*+=_~<>[]{}|\")\n",
        "        ))\n",
        "\n",
        "        self.char_to_num = {char: idx for idx, char in enumerate(self.char_list)}\n",
        "        self.num_to_char = {idx: char for idx, char in enumerate(self.char_list)}\n",
        "\n",
        "        self.word_images = []\n",
        "        self.word_labels = []\n",
        "        self.model = None\n",
        "        self.prediction_model = None\n",
        "        self.history = None\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Ensure text only contains valid characters\"\"\"\n",
        "        return ''.join([c for c in text if c in self.char_to_num])\n",
        "\n",
        "    def load_ground_truth(self):\n",
        "        \"\"\"Load and process all ground truth DOCX files\"\"\"\n",
        "        ground_truth = {}\n",
        "        for docx_file in os.listdir(self.ground_truth_path):\n",
        "            if docx_file.endswith('.docx'):\n",
        "                student_name = os.path.splitext(docx_file)[0].strip()\n",
        "                try:\n",
        "                    doc = Document(os.path.join(self.ground_truth_path, docx_file))\n",
        "                    text = \" \".join([para.text for para in doc.paragraphs])\n",
        "                    ground_truth[student_name] = self.clean_text(text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {docx_file}: {e}\")\n",
        "        return ground_truth\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Enhanced image preprocessing pipeline\"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Adaptive thresholding\n",
        "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                     cv2.THRESH_BINARY_INV, 11, 2)\n",
        "\n",
        "        # Noise removal\n",
        "        kernel = np.ones((2,2), np.uint8)\n",
        "        processed = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def extract_words(self, image, student_name, ground_truth):\n",
        "        \"\"\"Improved word extraction with better contour detection\"\"\"\n",
        "        processed = self.preprocess_image(image)\n",
        "\n",
        "        # Find contours\n",
        "        contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        boxes = []\n",
        "        for cnt in contours:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            if w > 15 and h > 15 and w*h > 200:  # Filter small noise\n",
        "                boxes.append((x, y, w, h))\n",
        "\n",
        "        # Sort boxes left-to-right, top-to-bottom\n",
        "        boxes = sorted(boxes, key=lambda b: (b[1]//20, b[0]))\n",
        "\n",
        "        if student_name in ground_truth:\n",
        "            gt_words = ground_truth[student_name].split()\n",
        "            for i, (x, y, w, h) in enumerate(boxes[:min(len(boxes), len(gt_words))]):\n",
        "                # Extract and preprocess word image\n",
        "                word_img = image[y:y+h, x:x+w]\n",
        "                word_img = cv2.resize(word_img, (128, 32))  # Wider aspect ratio for words\n",
        "                word_img = cv2.cvtColor(word_img, cv2.COLOR_BGR2RGB)  # Ensure RGB format\n",
        "\n",
        "                self.word_images.append(word_img)\n",
        "                self.word_labels.append(gt_words[i])\n",
        "\n",
        "    def process_all_scripts(self):\n",
        "        \"\"\"Process all student scripts with enhanced error handling\"\"\"\n",
        "        ground_truth = self.load_ground_truth()\n",
        "        processed_count = 0\n",
        "\n",
        "        for file in os.listdir(self.student_scripts_path):\n",
        "            file_path = os.path.join(self.student_scripts_path, file)\n",
        "            student_name = os.path.splitext(file)[0].strip()\n",
        "\n",
        "            try:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    images = convert_from_path(file_path)\n",
        "                    for img in images:\n",
        "                        img_np = np.array(img)[..., :3]  # Ensure 3 channels\n",
        "                        self.extract_words(img_np, student_name, ground_truth)\n",
        "                        processed_count += 1\n",
        "                elif file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    img = cv2.imread(file_path)\n",
        "                    if img is not None:\n",
        "                        self.extract_words(img, student_name, ground_truth)\n",
        "                        processed_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "        print(f\"\\nProcessing Complete:\")\n",
        "        print(f\"- Processed {processed_count} files\")\n",
        "        print(f\"- Extracted {len(self.word_images)} word images\")\n",
        "        print(f\"- Ground truth labels: {len(self.word_labels)}\")\n",
        "\n",
        "        # Save dataset info\n",
        "        self.save_dataset_info()\n",
        "\n",
        "    def save_dataset_info(self):\n",
        "        \"\"\"Save dataset statistics and sample images\"\"\"\n",
        "        os.makedirs(\"dataset_info\", exist_ok=True)\n",
        "\n",
        "        # Save label distribution\n",
        "        label_lengths = [len(label) for label in self.word_labels]\n",
        "        plt.hist(label_lengths, bins=20)\n",
        "        plt.title(\"Label Length Distribution\")\n",
        "        plt.savefig(\"dataset_info/label_lengths.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # Save character frequency\n",
        "        char_freq = {}\n",
        "        for label in self.word_labels:\n",
        "            for char in label:\n",
        "                char_freq[char] = char_freq.get(char, 0) + 1\n",
        "        pd.Series(char_freq).sort_values(ascending=False).to_csv(\"dataset_info/char_frequency.csv\")\n",
        "\n",
        "        # Save sample images\n",
        "        os.makedirs(\"dataset_info/samples\", exist_ok=True)\n",
        "        for i in range(min(20, len(self.word_images))):\n",
        "            cv2.imwrite(f\"dataset_info/samples/sample_{i}.png\", cv2.cvtColor(self.word_images[i], cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    def build_model(self, max_label_length=20):\n",
        "        \"\"\"Enhanced CRNN model architecture with configurable sequence length\"\"\"\n",
        "        input_img = Input(shape=(32, 128, 3))  # Matches our new image dimensions\n",
        "\n",
        "        # CNN Layers\n",
        "        x = Conv2D(32, (3,3), activation='relu', padding='same')(input_img)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2,2))(x)  # 64x64\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((2,2))(x)  # 32x32\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D((1,2))(x)  # 32x16\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        # Prepare for RNN - Now outputs 32 timesteps (from original width 128)\n",
        "        x = Reshape((32, 128*4))(x)\n",
        "        x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2))(x)\n",
        "        x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2))(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(len(self.char_list), activation='softmax')(x)\n",
        "\n",
        "        # CTC Loss\n",
        "        labels = Input(shape=[None], dtype='int32')\n",
        "        input_length = Input(shape=[1], dtype='int32')\n",
        "        label_length = Input(shape=[1], dtype='int32')\n",
        "\n",
        "        loss_out = Lambda(self.ctc_loss, output_shape=(1,), name='ctc')(\n",
        "            [output, labels, input_length, label_length]\n",
        "        )\n",
        "\n",
        "        # Create and compile model\n",
        "        self.model = Model(\n",
        "            inputs=[input_img, labels, input_length, label_length],\n",
        "            outputs=loss_out\n",
        "        )\n",
        "        self.model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                         loss={'ctc': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        self.prediction_model = Model(input_img, output)\n",
        "        self.max_label_length = max_label_length\n",
        "\n",
        "    def ctc_loss(self, args):\n",
        "        \"\"\"CTC loss function with sequence length validation\"\"\"\n",
        "        y_pred, labels, input_length, label_length = args\n",
        "\n",
        "        # Calculate the maximum sequence length\n",
        "        max_seq_len = K.shape(y_pred)[1]\n",
        "\n",
        "        # Ensure label lengths don't exceed input lengths\n",
        "        label_length = K.minimum(label_length, max_seq_len)\n",
        "\n",
        "        return K.ctc_batch_cost(\n",
        "            labels,\n",
        "            y_pred,\n",
        "            input_length,\n",
        "            label_length\n",
        "        )\n",
        "\n",
        "    def decode_predictions(self, pred):\n",
        "        input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "        results = K.get_value(K.ctc_decode(\n",
        "            pred,\n",
        "            input_length=input_len,\n",
        "            greedy=True\n",
        "        )[0][0])\n",
        "\n",
        "        output_text = []\n",
        "        for res in results:\n",
        "            res = res[res != -1]  # Remove padding\n",
        "            current_text = ''.join([self.num_to_char.get(int(k), '') for k in res])\n",
        "            output_text.append(current_text)\n",
        "\n",
        "        return output_text\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Calculate both word-level and character-level accuracy\"\"\"\n",
        "        word_correct = 0\n",
        "        char_correct = 0\n",
        "        char_total = 0\n",
        "\n",
        "        for true, pred in zip(y_true, y_pred):\n",
        "            # Word-level accuracy\n",
        "            if true == pred:\n",
        "                word_correct += 1\n",
        "\n",
        "            # Character-level accuracy\n",
        "            min_len = min(len(true), len(pred))\n",
        "            for t, p in zip(true[:min_len], pred[:min_len]):\n",
        "                if t == p:\n",
        "                    char_correct += 1\n",
        "                char_total += 1\n",
        "            char_total += abs(len(true) - len(pred))\n",
        "\n",
        "        word_acc = word_correct / len(y_true)\n",
        "        char_acc = char_correct / char_total if char_total > 0 else 0\n",
        "\n",
        "        return word_acc, char_acc\n",
        "\n",
        "    def prepare_training_data(self):\n",
        "        \"\"\"Prepare data with automatic length adjustment\"\"\"\n",
        "        # Filter out labels that are too long\n",
        "        filtered_images = []\n",
        "        filtered_labels = []\n",
        "        for img, label in zip(self.word_images, self.word_labels):\n",
        "            if len(label) <= self.max_label_length:\n",
        "                filtered_images.append(img)\n",
        "                filtered_labels.append(label)\n",
        "\n",
        "        print(f\"\\nFiltered {len(self.word_images)-len(filtered_images)} labels longer than {self.max_label_length} characters\")\n",
        "\n",
        "        X = np.array(filtered_images, dtype=np.float32) / 255.0\n",
        "        y = [np.array([self.char_to_num[c] for c in word], dtype=np.int32)\n",
        "             for word in filtered_labels]\n",
        "\n",
        "        # Pad sequences\n",
        "        max_label_len = max(len(word) for word in filtered_labels)\n",
        "        y_padded = np.zeros((len(y), max_label_len), dtype=np.int32)\n",
        "        for i, seq in enumerate(y):\n",
        "            y_padded[i, :len(seq)] = seq\n",
        "\n",
        "        # Calculate sequence lengths - now matches model's output timesteps (32)\n",
        "        input_length = np.ones((X.shape[0], 1), dtype=np.int32) * 32\n",
        "        label_length = np.array([[len(word)] for word in filtered_labels], dtype=np.int32)\n",
        "\n",
        "        return X, y_padded, input_length, label_length\n",
        "\n",
        "    def train(self, epochs=50, batch_size=32):\n",
        "        \"\"\"Enhanced training process with automatic length handling\"\"\"\n",
        "        # Prepare data\n",
        "        X, y_padded, input_length, label_length = self.prepare_training_data()\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val, il_train, il_val, ll_train, ll_val = train_test_split(\n",
        "            X, y_padded, input_length, label_length, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
        "            ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss'),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, monitor='val_loss')\n",
        "        ]\n",
        "\n",
        "        # Train\n",
        "        self.history = self.model.fit(\n",
        "            [X_train, y_train, il_train, ll_train],\n",
        "            np.zeros(X_train.shape[0]),\n",
        "            validation_data=(\n",
        "                [X_val, y_val, il_val, ll_val],\n",
        "                np.zeros(X_val.shape[0])\n",
        "            ),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Calculate final accuracy\n",
        "        self.evaluate_performance(X_val, y_val)\n",
        "\n",
        "    def evaluate_performance(self, X_test, y_test):\n",
        "        \"\"\"Comprehensive performance evaluation\"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = self.prediction_model.predict(X_test)\n",
        "        decoded = self.decode_predictions(y_pred)\n",
        "\n",
        "        # Convert true labels to text\n",
        "        y_true_text = []\n",
        "        for seq in y_test:\n",
        "            text = ''.join([self.num_to_char.get(int(k), '') for k in seq if k != 0])\n",
        "            y_true_text.append(text)\n",
        "\n",
        "        # Calculate accuracies\n",
        "        word_acc, char_acc = self.calculate_accuracy(y_true_text, decoded)\n",
        "\n",
        "        print(\"\\nFinal Evaluation:\")\n",
        "        print(f\"Word-level Accuracy: {word_acc:.4f}\")\n",
        "        print(f\"Character-level Accuracy: {char_acc:.4f}\")\n",
        "\n",
        "        # Save sample predictions\n",
        "        results = pd.DataFrame({\n",
        "            'Actual': y_true_text,\n",
        "            'Predicted': decoded,\n",
        "            'Correct': [a == p for a, p in zip(y_true_text, decoded)]\n",
        "        })\n",
        "        results.to_csv('predictions.csv', index=False)\n",
        "\n",
        "        # Plot training history\n",
        "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Training History')\n",
        "        plt.legend()\n",
        "        plt.savefig('training_history.png')\n",
        "        plt.close()\n",
        "\n",
        "    def predict(self, image_path):\n",
        "        \"\"\"Predict text from a new image\"\"\"\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            return \"\"\n",
        "\n",
        "        # Preprocess\n",
        "        img = cv2.resize(img, (128, 32))  # Match training dimensions\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = np.expand_dims(img, axis=0) / 255.0\n",
        "\n",
        "        # Predict\n",
        "        pred = self.prediction_model.predict(img)\n",
        "        decoded = self.decode_predictions(pred)\n",
        "\n",
        "        return decoded[0]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ocr = HandwrittenOCR(\n",
        "        student_scripts_path=\"/content/drive/MyDrive/exam_scripts/project_folder/student_scripts\",\n",
        "        ground_truth_path=\"/content/drive/MyDrive/exam_scripts/project_folder/ground_truth\"\n",
        "    )\n",
        "\n",
        "    # Process data and train with increased max label length\n",
        "    ocr.process_all_scripts()\n",
        "    ocr.build_model(max_label_length=20)\n",
        "    ocr.train(epochs=50)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz1izPM49qLT",
        "outputId": "6d5d177a-5fe2-440c-b157-cdc09b82c045"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing Complete:\n",
            "- Processed 106 files\n",
            "- Extracted 23606 word images\n",
            "- Ground truth labels: 23606\n",
            "\n",
            "Filtered 0 labels longer than 20 characters\n",
            "Epoch 1/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 23.7901"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 45ms/step - loss: 23.7807 - val_loss: 98.3681 - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 16.2348"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - loss: 16.2352 - val_loss: 17.7595 - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 16.2917"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 48ms/step - loss: 16.2915 - val_loss: 16.3308 - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 16.1365"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 16.1365 - val_loss: 16.1161 - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 45ms/step - loss: 15.9492 - val_loss: 16.2658 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 45ms/step - loss: 16.0558 - val_loss: 16.5841 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 15.9921"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 47ms/step - loss: 15.9921 - val_loss: 15.9355 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 43ms/step - loss: 15.9747 - val_loss: 15.9893 - learning_rate: 5.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m589/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 15.8427"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 47ms/step - loss: 15.8428 - val_loss: 15.8871 - learning_rate: 5.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 15.7909"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 42ms/step - loss: 15.7910 - val_loss: 15.7183 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - loss: 15.6971 - val_loss: 16.1022 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 44ms/step - loss: 15.8388 - val_loss: 16.6366 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 46ms/step - loss: 15.7098 - val_loss: 15.8529 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m589/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 15.7894"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - loss: 15.7886 - val_loss: 15.6145 - learning_rate: 2.5000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 46ms/step - loss: 15.6677 - val_loss: 15.6190 - learning_rate: 2.5000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 45ms/step - loss: 15.5565 - val_loss: 15.6373 - learning_rate: 2.5000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 15.4676"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - loss: 15.4679 - val_loss: 15.5988 - learning_rate: 2.5000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - loss: 15.6025 - val_loss: 15.6162 - learning_rate: 2.5000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - loss: 15.5932 - val_loss: 15.6153 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 49ms/step - loss: 15.4966 - val_loss: 15.6302 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 15.6059"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - loss: 15.6055 - val_loss: 15.5988 - learning_rate: 1.2500e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 45ms/step - loss: 15.4528 - val_loss: 15.6037 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - loss: 15.3998 - val_loss: 15.6048 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 15.4468"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 15.4468 - val_loss: 15.5976 - learning_rate: 6.2500e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m590/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 15.5327"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 15.5323 - val_loss: 15.5874 - learning_rate: 6.2500e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 15.3858 - val_loss: 15.6011 - learning_rate: 6.2500e-05\n",
            "Epoch 27/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 46ms/step - loss: 15.3293 - val_loss: 15.6086 - learning_rate: 6.2500e-05\n",
            "Epoch 28/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 15.3502 - val_loss: 15.5990 - learning_rate: 6.2500e-05\n",
            "Epoch 29/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 47ms/step - loss: 15.3372 - val_loss: 15.6125 - learning_rate: 3.1250e-05\n",
            "Epoch 30/50\n",
            "\u001b[1m591/591\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 46ms/step - loss: 15.4543 - val_loss: 15.6042 - learning_rate: 3.1250e-05\n",
            "\u001b[1m148/148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step\n",
            "\n",
            "Final Evaluation:\n",
            "Word-level Accuracy: 0.0055\n",
            "Character-level Accuracy: 0.0191\n"
          ]
        }
      ]
    }
  ]
}